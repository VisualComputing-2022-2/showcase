[{"id":0,"href":"/showcase/docs/shortcodes/ames/ames/","title":"Ames","section":"Shortcodes","content":" Ames Ilussions # Adelbert Ames Jr. # Adelbert Ames Jr. (August 19, 1880 – July 3, 1955) was an American scientist who made contributions to physics, physiology, ophthalmology, psychology, and philosophy. He pioneered the study of physiological optics at Dartmouth College, serving as a research professor, then as director of research at the Dartmouth Eye Institute. He conducted important research into aspects of binocular vision, including cyclophoria and aniseikonia. Ames is perhaps best known for constructing illusions of visual perception, most notably the Ames room and the Ames window. He was a leading light in the Transactionalist School of psychology and also made contributions to social psychology.\nAmes is perhaps best known for his eponymous room, window, and chair. These were called \u0026ldquo;equivalent configurations\u0026rdquo; by Ittelson (1952), defined as \u0026ldquo;configurations [in which] identical \u0026lsquo;incoming messages\u0026rsquo; can come from different external physical arrangements. In the absence of other information, equivalent configurations will be perceived as identical, no matter how different they be physically\u0026rdquo;. Ames also developed the concept of \u0026ldquo;transactional ambiguity\u0026rdquo; holding that \u0026ldquo;mental set\u0026rdquo; or expectation could materially affect one\u0026rsquo;s perception of visual and other stimuli, as with the Ames trapezoid. This hypothesis extended the impact of mental set from the widely believed impact on one\u0026rsquo;s conclusions about stimuli (the eyewitness phenomenon) to actual perception of the stimuli itself. If true, it calls seriously into question the value of eyewitness reports even by individuals with no prejudices about their observations. In 1941 Ames began to make notes in the morning on his transactional analysis of perception. In 1960 his collaborator Hadley Cantril published an edited selection from these notes, with a Preface, and included Ames\u0026rsquo; correspondence with John Dewey. In 1954, Ames was awarded an honorary Doctor of Laws by Dartmouth. In 1955 he won the Tillyer Medal, awarded by the Optical Society of America. Ames died on July 3, 1955, and was buried at the Dartmouth Cemetery. His name, and that of his siblings, is also inscribed on the gravestone of his parents at the Hildreth family cemetery in Lowell.\nAmes Window # One of the most important things when creating the ames window is to adjust the camera orthographically, since if you use a perspective camera it will be possible to perceive depth. To make this concept clearer, the difference between both types of camera is shown below. Commands Key Description e Sphere s Stick m Material IRL Ames Room # Ames Room # Commands Key Description b Boxes c Cones t Torus r Reset f Compare EasyCam Controls Camera Conclusions # When three-dimensional scenes are built on two-dimensional canvases (the screen in this case) it is necessary to consider several concepts, among them are the frontal, oblique and aerial perspectives, each one of them with its characteristic vanishing points, in addition to the important line of the horizon. The proper use of the above concepts allows realistically capture a three-dimensional scenario. In the construction of simulated scenarios, it is possible to save resources through perceptive games that trick the viewer into not being able to notice any difference between a complete scenario and another that simply appears to be. The perception of reality is subjective, selective, and temporary so that, if a conflict or knot appears that does not allow an individual to fully understand it, a dialogic process begins with the environment, trying to make a readjustment in his experience. That is why, despite creating images that are not totally faithful, they are totally real, since it is the individual impression that builds the true perception of the world. "},{"id":1,"href":"/showcase/docs/shortcodes/illusions/illusions/","title":"Illusions","section":"Shortcodes","content":" Illusions # \u0026ldquo;Stepping Feet\u0026rdquo; Motion Illusion # It was demonstrated by Stuart Anstis in 2003.\nThe edges of the light ‘foot’ merge with the light bars and are only visible when they traverse the dark bars. So, half of the time there really is no motion cue, and perception goes into default, i.e., no motion. For the dark foot the same holds, only at alternate times.\nWith reduced contrast of the grating, equiluminance of edges and grating is no longer present, so the effect disappears\nThe button “Grid” allows to hide the black bars in order to make the optical effect disappear and thus check the uniform movement of each foot\nPyramid Illusion (Vasarely Illusion) # It was first demonstrated by Martinez-Conde \u0026amp; Macknik at the 2001 Soc Neurosci Meeting.\nThis is a striking perceptual effect related to all phenomena involving lateral inhibition. The effect occurs when concentric squares (or other geometrical figures) of decreasing size and luminance are stacked on top of another.\nIf you consider arrangements of antagonistic centre-surround ganglion cell receptive fields convolving the image. Imagine a cell at a corner with its centre in the lighter patch, it will be inhibited by 1/4 surround from lighter patches and by 3/4 from darker patches. Voilà, the ganglion cells at the edges signal more brightness. However: for perception the ganglion cell information is integrated, and the heavy luminance distortion removed\nIt is why the inverse transformation applied to correct the retinal convolution breaks down in the pyramid\nThe bar located at the top allows you to modify the number of overlapping squares in order to gradually obtain the illusion\n\u0026ldquo;Breathing Square\u0026rdquo; Illusion # This illusion is one of several effects in which a rigid object is shown moving behind small apertures. The object appears to change size or shape as it moves. In this case the apertures become progressively narrower and wider as a square rotates behind them, and the square appears to expand and contract rhythmically that is why is called a breathe.\nIt is possible to make the squares that move from each of the corners disappear by means of the \u0026ldquo;corners\u0026rdquo; button. In this way the so-called \u0026ldquo;breathing\u0026rdquo; disappears, showing what is simply a normal rotation of a square with fixed size\nWhite Xmas Illusion # In this illusion there are only three different colors however apparently each of the trees has a different shade of green.\nThe difference between each of the two trees is only that one of them is under the yellow bars while the other one is under the green bars.\nIn this case it is possible to make the horizontal bars disappear by means of the \u0026ldquo;bars\u0026rdquo; button located at the top. When the bars disappear, it is possible to verify the color parity of the trees.\nFlash-Lag Effect # This illusion is an example of the “flash-lag” effect.\nWhere the two lines nearly meet, the flashing line appears to lag a little behind the other line.\nThe flashing line seem to have a different angle, where they meet there’s a kink.\nOur mental perception and planning mechanisms need to consider the delays in afference, computation \u0026amp; efference. Thus, moving objects are perceived a bit ahead of their assumed trajectory; the flash (because it is stationary) is not. Consequently, one perceives a positional disparity between briefly flashed stationary and moving objects.\nFor this illusion we have a bar with which we can control the speed of the rotation of the line that is always visible When the speed is higher the effect is much clearer\n"},{"id":2,"href":"/showcase/docs/shortcodes/masking/","title":"Masking","section":"Shortcodes","content":" Image Kernels # What is a Convolution? # In image processing, convolution is the process of transforming an image by applying a kernel over each pixel and its local neighbors across the entire image.\nThe Convolution Process involves those two steps.\nIt places the Kernel Matrix over each pixel of the image, multiplies each value of the Kernel with the corresponding pixel it is over. Sums the resulting multiplied values and returns the resulting value as the new value of the center pixel. This process is repeated for each pixel in the image. What is a Kernel? # A kernel is in fact a matrix with an M x N dimension that is smaller than the image matrix. The kernel is also known as the convolution matrix and it\u0026rsquo;s used for the tasks like blurring, sharpening, edge-detection and similar image processing tasks.\nKernels Used in this Example: # The Prewitt Operator: Can be used for both vertical and horizontal edge detection.\nVertical \\( \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; -1\\\\ 1 \u0026amp; 0 \u0026amp; -1\\\\ 1 \u0026amp; 0 \u0026amp; -1 \\end{bmatrix} \\) Horizontal \\( \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0\\\\ -1 \u0026amp; -1 \u0026amp; -1 \\end{bmatrix} \\) The Sobel Operator: The Sobel operator emphasizes the edges more than the Prewitt operator by replacing 1’s in the central column with 2’s.\nVertical \\( \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; -1\\\\ 2 \u0026amp; 0 \u0026amp; -2\\\\ 1 \u0026amp; 0 \u0026amp; -1 \\end{bmatrix} \\) Horizontal \\( \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0\\\\ -1 \u0026amp; -2 \u0026amp; -1 \\end{bmatrix} \\) Box: The box kernel is a simple filter that calculates the mean of the pixels in the area covered by the filter. This also has a smoothing effect.\n\\( \\frac{1}{9} \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1\\\\ 1 \u0026amp; 1 \u0026amp; 1\\\\ 1 \u0026amp; 1 \u0026amp; 1 \\end{bmatrix} \\) Gaussian: The Gaussian kernel is a filter that calculates the weighted mean of the pixels in the area covered by the filter, it\u0026rsquo;s used to blur an image.\n\\( \\frac{1}{16} \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 1\\\\ 2 \u0026amp; 4 \u0026amp; 2\\\\ 1 \u0026amp; 2 \u0026amp; 1 \\end{bmatrix} \\) Sharpen: The sharpen kernel is a filter thah emphasizes differences in adjacent pixel values. This makes the image look more vivid.\n\\( \\begin{bmatrix} 0 \u0026amp; -1 \u0026amp; 0\\\\ -1 \u0026amp; 5 \u0026amp; -1\\\\ 0 \u0026amp; -1 \u0026amp; 0 \\end{bmatrix} \\) Shortcuts Shortcut Description V Vertical Edge H Horizontal Edge E Edge Kernel B Box Kernel S Sharpen Kernel G Gaussian Kernel Z Sobel Kernel F Reset Image C + Brightness D - Brightness About the histogram # As you can see, as the different filters (kernels) area applied, the color composition of the image changes as well. This is also true for changes in the brightness.\nThe histogram shows the frequency of the colors in the image. Note that as the image gets darker, we get more colors at the left of the diagram, and as the image gets brighter, we get more colors at the right.\n"},{"id":3,"href":"/showcase/docs/shortcodes/shaders/1.Coloring-and-Ascii-art/","title":"1. Coloring and Ascii Art","section":"Shaders","content":" Shaders # Color Blending # ADD: It produces the new color by adding the colors of both the pixels. DIFFERENCE: It subtracts colors from the underlying image. DARKEST: It uses only the darker color of the two pixels. LIGHTEST: It uses only the lighter color of the two pixels. Shortcuts Shortcut Description R Red Circle G Green Circle B Blue Circle Click Change Blending Glitch Effect Shader # Basado en el código de este video La idea del shader es tener un offset para la coordenada en x donde se dibuja el R y el B. Presione r para limpiar el canvas\nCódigo (shader): void main() { vec2 uv = vTexCoord; uv.y = 1.0 - uv.y; vec2 offset = vec2(noise * 0.05, 0.0); vec3 col; col.r = texture2D(texture, uv + offset).r; col.g = texture2D(texture, uv).g; col.b = texture2D(texture, uv - offset).b; gl_FragColor = vec4(col, 1.0); } Si se limita y varía el offset con el tiempo, se tiene el efecto de \u0026ldquo;glitch\u0026rdquo; Presione r para limpiar el canvas\nAscii Art # Ascii art visual application (software): Basado en el tutorial de The Coding Train\nCommands Command Description C Change image "},{"id":4,"href":"/showcase/docs/shortcodes/shaders/2.Texturing/","title":"2. Texturing","section":"Shaders","content":" Texture Sampling # El modelo de color HSV es una transformación no lineal de color RGB y se usa para progreciones de color\nEn la imagen podemos ver el cono del modelo HSV, podemos movernos a través del mismo con un vector de 3 dimensiones de forma:\n0º = RGB(1, 0, 0) 60º = RGB(1, 1, 0) 120º = RGB(0, 1, 0) 180º = RGB(0, 1, 1) 240º = RGB(0, 0, 1) 300º = RGB(1, 0, 1) 360º = 0º A continuación se muestra un ejemplo de esto:\nInstrucciones\nEn el selector, escoja el grado de rotación en el cono del modelo HSV Código (shader): precision mediump float; uniform int selectedTool; uniform int colorTexture; uniform sampler2D texture; uniform vec2 mousePos; varying vec2 texcoords2; float map(float value, float min1, float max1, float min2, float max2) { return min2 + (value - min1) * (max2 - min2) / (max1 - min1); } vec3 ahsv(vec3 c) { vec4 K = vec4(1.0, -1.0 / 3.0, 0.0, -1.0); vec4 p = mix(vec4(c.bg, K.wz), vec4(c.gb, K.xy), step(c.b, c.g)); vec4 q = mix(vec4(p.xyw, c.r), vec4(c.r, p.yzx), step(p.x, c.r)); float d = q.x - min(q.w, q.y); float e = 1.0e-10; return vec3(abs(q.z + (q.w - q.y) / (10.0 * d + e)), d / (q.x + e), q.x); } vec3 ahsl(vec3 c) { vec3 outputColor = vec3(0.0); outputColor.x = c.x; outputColor.z = c.z * (1.0 - (c.y / 2.0)); if(outputColor.z == 0.0 || outputColor.z == 1.0) { outputColor.y = 0.0; } else { outputColor.y = (c.x - outputColor.z) / min(outputColor.z, 1.0 - outputColor.z); } return outputColor; } void main() { vec4 texel = texture2D(texture, texcoords2); vec4 outputValue; if(selectedTool == 0) { outputValue = texel; } else if(selectedTool == 1) { vec3 hsv = ahsv(texel.rgb); outputValue = vec4(hsv.rgb, 1.0); } else if(selectedTool == 2) { vec3 hsv = ahsv(texel.rgb); vec3 hsl = ahsl(hsv); outputValue = vec4(hsl.rgb, 1.0); } if(colorTexture == 0) { outputValue.rgb = outputValue.rgb * 1.0; } else if(colorTexture == 1) { outputValue.rgb = outputValue.rgb * vec3(1.0, 0.0, 0.0); } else if(colorTexture == 2) { outputValue.rgb = outputValue.rgb * vec3(1.0, 1.0, 0.0); } else if(colorTexture == 3) { outputValue.rgb = outputValue.rgb * vec3(0.0, 1.0, 0.0); } else if(colorTexture == 4) { outputValue.rgb = outputValue.rgb * vec3(0.0, 1.0, 1.0); } else if(colorTexture == 5) { outputValue.rgb = outputValue.rgb * vec3(0.0, 0.0, 1.0); } else if(colorTexture == 6) { outputValue.rgb = outputValue.rgb * vec3(1.0, 0.0, 1.0); } gl_FragColor = outputValue; } HSL, tiene dos vertices que representan el blanco y el negro, el ángulo se corresponde con el Hue, la distancia con la Saturation y la distancia al eje blanco-negro con el Lightness\ntransformaciones a HSV y HSL\nInstrucciones\nEn el selector, escoja la trasformación HSV, HSL o la imagen original "},{"id":5,"href":"/showcase/docs/shortcodes/shaders/3.Image-Processing/","title":"3. Image Processing","section":"Shaders","content":" Image processing Tool # En esta herramienta se implementaron diferentes matrices de convolucion para poder procesar una imagen y se genero un foco para poder ver donde se aplica en un area determinada por el slider.\nInstrucciones\nEn el primer seleccionador, escoja el filtro a aplicar Escoja foco en la caja de selección si desea aplicar un foco en el filtro.Una vez se aplique el foco, el filtro se aplicará sobre la seccion de la imagen debajo del mouse y no en toda la imagen. Utilice el desilizador para amuentar o disminuir el tamaño del foco Código (shader): #ifdef GL_ES precision mediump float; #endif uniform sampler2D texture; uniform vec2 texOffset; // holds the 3x3 kernel uniform float mask[9]; uniform vec2 u_mouse; uniform vec2 u_resolution; uniform bool foco; uniform float radio; // we need our interpolated tex coord varying vec2 texcoords2; void main() { vec2 xy = gl_FragCoord.xy - u_mouse.xy; float R = radio; float h = 40.; float hr = R * sqrt(1. - ((R - h) / R) * ((R - h) / R)); float r = sqrt(xy.x * xy.x + xy.y * xy.y); vec2 new_xy = r \u0026lt; hr ? xy * (R - h) / sqrt(R * R - r * r) : xy; // 1. Use offset to move along texture space. // In this case to find the texcoords of the texel neighbours. vec2 tc0 = texcoords2 + vec2(-texOffset.s, -texOffset.t); vec2 tc1 = texcoords2 + vec2(0.0, -texOffset.t); vec2 tc2 = texcoords2 + vec2(texOffset.s, -texOffset.t); vec2 tc3 = texcoords2 + vec2(-texOffset.s, 0.0); // origin (current fragment texcoords) vec2 tc4 = texcoords2 + vec2(0.0, 0.0); vec2 tc5 = texcoords2 + vec2(texOffset.s, 0.0); vec2 tc6 = texcoords2 + vec2(-texOffset.s, texOffset.t); vec2 tc7 = texcoords2 + vec2(0.0, texOffset.t); vec2 tc8 = texcoords2 + vec2(texOffset.s, texOffset.t); // 2. Sample texel neighbours within the rgba array vec4 rgba[9]; rgba[0] = texture2D(texture, tc0); rgba[1] = texture2D(texture, tc1); rgba[2] = texture2D(texture, tc2); rgba[3] = texture2D(texture, tc3); rgba[4] = texture2D(texture, tc4); rgba[5] = texture2D(texture, tc5); rgba[6] = texture2D(texture, tc6); rgba[7] = texture2D(texture, tc7); rgba[8] = texture2D(texture, tc8); // 3. Apply convolution kernel vec4 convolution; for(int i = 0; i \u0026lt; 9; i++) { convolution += rgba[i] * mask[i]; } if(foco) { gl_FragColor = r \u0026lt; hr ? vec4(convolution.rgb, 1.0) : texture2D(texture, texcoords2); } else { gl_FragColor = vec4(convolution.rgb, 1.0); } } Video processing Tool # Gracias al mismo principio se puede usar en videos ya que estos son una serie de imagenes solo se debe aplicar la máscara a cada frame.\nInstrucciones\nEn el primer seleccionador, escoja el filtro a aplicar Escoja foco en la caja de selección si desea aplicar un foco en el filtro.Una vez se aplique el foco, el filtro se aplicará sobre la seccion de la imagen debajo del mouse y no en toda la imagen. Utilice el desilizador para amuentar o disminuir el tamaño del foco "},{"id":6,"href":"/showcase/docs/shortcodes/shaders/3.Image-Processing/Magnifier-Tool/","title":"Magnifier Tool","section":"3. Image Processing","content":" Magnifier Tool # Esta apliación permite utilizar un fragment shader para amplificar el tamaño de la imagen y de un video alreder de un foco.\nInstrucciones\nEscoja video o imagenen la caja de selección Utilice el desilizador para amuentar o disminuir el tamaño del foco Código (shader): #ifdef GL_ES precision mediump float; #endif uniform sampler2D texture; uniform vec2 texOffset; // holds the 3x3 kernel uniform float mask[9]; uniform vec2 u_mouse; uniform vec2 u_resolution; uniform int option; uniform bool region; uniform float radio; // we need our interpolated tex coord varying vec2 texcoords2; void main() { vec2 xy = gl_FragCoord.xy - u_mouse.xy; float R = radio; float h = 40.; float hr = R * sqrt(1. - ((R - h) / R) * ((R - h) / R)); float r = sqrt((xy.x * xy.x) + (xy.y * xy.y)); vec2 new_xy = r \u0026lt; hr ? xy * (R - h) / sqrt(R * R + r * r) : xy; gl_FragColor = texture2D(texture, (new_xy.xy + u_mouse.xy) / u_resolution.xy); } "},{"id":7,"href":"/showcase/docs/shortcodes/shaders/4.Procedural-Texturing/","title":"4. Procedural Texturing","section":"Shaders","content":" Procedural texturing # Se crean las texturas mediante un fragment shader el cual recibe la información pixel a pixel, y de acuerdo a sus coordenadas ST se mapea un color sobre la figura seleccionada. además se modifican las coordenadas de cada pixel de manera que se devuelva su parte fraccional\n\\[ f(x) = x - floor(x) \\] Esta función es luego usada para asignar el color de cada pixel sobre la figura:\nst = tile(st,u_zoom*0.5); gl_FragColor = vec4(vec3(step(st.x,st.y)),1.0); Este es el proceso mediante el cual se generan texturas procedimentales. Para crear nuevas texturas se pueden utilizar nuevas funciones matemáticas. A continuación, se muestran varios ejempos de texturas procedimentales sobre varias figuras.\nInstrucciones\nEn el primer selector, selecciona la figura sobre la cual aplicar la textura En el segundo slector, selecciona la textura a aplicar Mueve el mouse para cambiar el tamaño de la textura Control de la camara mediante EasyCam Código de la textura brick (shader): // Author @patriciogv ( patriciogonzalezvivo.com ) - 2015 #ifdef GL_ES precision mediump float; #endif uniform vec2 u_resolution; uniform float u_time; uniform float u_zoom; vec2 brickTile(vec2 _st, float _zoom){ _st *= _zoom; // Here is where the offset is happening _st.x += step(1., mod(_st.y,2.0)) * 0.5; return fract(_st); } float box(vec2 _st, vec2 _size){ _size = vec2(0.5)-_size*0.5; vec2 uv = smoothstep(_size,_size+vec2(1e-4),_st); uv *= smoothstep(_size,_size+vec2(1e-4),vec2(1.0)-_st); return uv.x*uv.y; } void main(void){ vec2 st = gl_FragCoord.xy/u_resolution.xy; vec3 color = vec3(0.0); // Modern metric brick of 215mm x 102.5mm x 65mm // http://www.jaharrison.me.uk/Brickwork/Sizes.html // st /= vec2(2.15,0.65)/1.5; // Apply the brick tiling st = brickTile(st,u_zoom*0.5); color = vec3(box(st,vec2(0.9))); // Uncomment to see the space coordinates // color = vec3(st,0.0); gl_FragColor = vec4(color,1.0); } "},{"id":8,"href":"/showcase/docs/shortcodes/shaders/5.Non-Euclidean-Geometry/5.NonEuclideanGeometry/","title":"5. Non Euclidean Geometry","section":"Shaders","content":" Non Euclidean Geometry # Different independently generated three-dimensional spaces are linked so that they can be seen on different faces of a polygon. The idea is to make a separate rendering of each of the objects to be observed from the same point of view.\nShortcuts Shortcut Description C Cube O Octahedron Mouse Camera Control Shader precision mediump float; uniform sampler2D text; uniform vec2 u_res; void main() { vec2 st = gl_FragCoord.xy / u_res; gl_FragColor = texture2D(text, vec2(st.s, 1.0 - st.t)); } References # The Book of Shaders https://thebookofshaders.com/03/ Khronos Group https://registry.khronos.org/OpenGL-Refpages/gl4/html/gl_FragCoord.xhtml "},{"id":9,"href":"/showcase/docs/shortcodes/shaders/6.Lightning/","title":"6. Lightning","section":"Shaders","content":" Lighting # Diffuse Reflection # This type of reflection allows objects to be seen in an ideal matte tone. This is thanks to Lambert\u0026rsquo;s law of cosines.\nThe concept was introduced in 1760 in the book photometry written by Johann Heinrich Lambert Shader void main() { mediump float light = dot(vNormal, uLight1[1].xyz); gl_FragColor = vColor * vec4(light, light, light, 1); } Reference # Dev Community - WebGL: Diffuse Lighting https://dev.to/ndesmic/webgl-engine-from-scratch-part-7-diffuse-lighting-3in8 "},{"id":10,"href":"/showcase/docs/shortcodes/shaders/7.Conclusions/","title":"7. Conclusions","section":"Shaders","content":" Conclusiones # Los shaders permiten que la computación gráfica sea más rápida y efectiva. Esto se puede ver reflejado en el procesamiento de imágenes, en el cual, para aplicar un filtro, es necesario hacer transformaciones pixel a pixel. En algunas imágenes estos son millones de tareas por realizar. Por tal motivo, paralelizar estos procesos ha permitido que la apliación de filtros sea más rápida, previniendo demoras que puedan entorpecer la experiencia de usuario. Además, se pemite trabajar con texturas, demostrando que, sin necesidad de tener ninguna imagen preliminar, únicamente basandose en la geometría de los objetos y de cada uno de sus pixeles (como su posisción, su luminosidad, su color) y de las funciones y propiedades matemáticas se pueden generar diversos patrones sobre los objetos.\n"}]